# Long-Text-Summarization
This study enhances text summarization techniques using advanced transformer models to handle complex
documents like those in the BIGPATENT dataset, which includes 1.3 million U.S. patent documents. We
focused on integrating self-attention mechanisms and coreference resolution to improve summary quality. Four
models—LED, DistilBART, BigBird-Pegasus, and LongT5—were fine-tuned and evaluated using ROUGE,
BLEU, and METEOR metrics. The results showed significant performance gains, with LED achieving a
ROUGE-1 score of 95.38% and DistilBART achieving 91.94% on BLEU, marking a 50% improvement over
previous studies. These findings confirm the effectiveness of our methodologies and emphasize the importance
of high computational resources. Future work could explore multi-lingual and multi-modal data and develop
user-friendly interfaces to broaden the models’ applicability.
